#!/usr/bin/env python3
# streamlit_app.py
"""
Streamlit App: Voice-to-Proverb Wisdom (Structured Output + TTS)
Record audio (via browser) ‚Üí Transcribe ‚Üí Retrieve proverbs ‚Üí 
LLM returns JSON schema: { "thai_proverb": "...", "explanation": "..." } ‚Üí Text-to-Speech

Usage:
  streamlit run streamlit_app.py

Models:
  Speech-to-Text: biodatlab/whisper-th-medium-combined (Thai Whisper)
  Text-to-Speech: edge-tts (Microsoft Neural Voice)
  Embeddings: BAAI/bge-m3
  LLM: Azure OpenAI
"""

import os
import json
import tempfile
import asyncio
import base64
import html

import streamlit as st
import torch
from dotenv import load_dotenv, find_dotenv

# =====================================================
# Page configuration
# =====================================================
st.set_page_config(
    page_title="Chinese Proverbs Wisdom",
    page_icon="üé§",
    layout="wide",
    initial_sidebar_state="expanded",
)

load_dotenv(find_dotenv(), override=True)

# =====================================================
# Cached loaders
# =====================================================
@st.cache_resource
def load_whisper_model(model_name: str = "biodatlab/whisper-th-medium-combined"):
    """
    Load Speech-to-Text model (Whisper).
    
    Popular Thai models:
    - "biodatlab/whisper-th-medium-combined" (recommended, default)
    - "openai/whisper-small"
    - "openai/whisper-base"
    - "charsiu/thai_male" (‡∏Å‡πâ‡∏≠‡∏´‡∏ô‡∏∂‡πà‡∏á alternative)
    """
    from transformers import pipeline

    device = 0 if torch.cuda.is_available() else "cpu"

    pipe = pipeline(
        task="automatic-speech-recognition",
        model=model_name,
        chunk_length_s=30,
        device=device,
    )
    return pipe


@st.cache_resource
def load_vector_store():
    """
    Load ChromaDB vector store (‡πÉ‡∏ä‡πâ CPU ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á double-free crash ‡∏ö‡∏ô WSL2).
    """
    try:
        from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
        from langchain_community.vectorstores import Chroma
    except ImportError:
        # Fallback ‡∏ñ‡πâ‡∏≤ deprecated
        from langchain_huggingface import HuggingFaceEmbeddings
        from langchain_chroma import Chroma

    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )

    vector_store = Chroma(
        embedding_function=embeddings,
        persist_directory="./chroma_db",
        collection_name="chinese_proverbs",
    )
    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})
    return vector_store, retriever


@st.cache_resource
def load_llm_client():
    """
    Load OpenAI client pointing to Azure endpoint.
    """
    from openai import OpenAI

    endpoint = "https://llm-4-vision.cognitiveservices.azure.com/openai/v1/"
    api_key = os.getenv("GPT_API_KEY")
    if not api_key:
        raise RuntimeError("Missing GPT_API_KEY in environment variables")

    client = OpenAI(base_url=endpoint, api_key=api_key)
    # model_name = "gpt-5-mini"
    model_name = "gpt-4.1"
    return client, model_name


# =====================================================
# Core functions
# =====================================================
def transcribe_audio(audio_bytes, whisper_pipe) -> str:
    """
    Transcribe audio (from st.audio_input) using Whisper.
    audio_bytes: bytes from st.audio_input()
    """
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
        tmp_file.write(audio_bytes)
        tmp_path = tmp_file.name

    try:
        result = whisper_pipe(
            tmp_path,
            generate_kwargs={"language": "<|th|>", "task": "transcribe"},
            batch_size=16,
        )
        return (result.get("text") or "").strip()
    finally:
        try:
            os.unlink(tmp_path)
        except OSError:
            pass


def retrieve_proverbs(query: str, retriever):
    """Retrieve relevant proverbs from vector store."""
    return retriever.invoke(query)


def _format_proverbs_context(docs) -> str:
    """Format retrieved documents as context string."""
    lines = []
    for i, d in enumerate(docs, 1):
        md = d.metadata or {}
        chinese = md.get("chinese", "")
        pinyin = md.get("pinyin", "")
        english = md.get("english", "")
        category = md.get("category", "")
        lines.append(
            f"{i}. {chinese} | {pinyin} | {english} | {category}".strip()
        )
    return "\n".join(lines)


def _extract_first_json_obj(text: str):
    # Brace-matching: ‡∏´‡∏≤ JSON object ‡∏Å‡πâ‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡πÅ‡∏ö‡∏ö‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤ regex
    if not text:
        return None
    start = text.find("{")
    if start == -1:
        return None
    depth = 0
    for i in range(start, len(text)):
        c = text[i]
        if c == "{":
            depth += 1
        elif c == "}":
            depth -= 1
            if depth == 0:
                return text[start:i+1]
    return None


def llm_structured_proverb(user_situation: str, docs, client_and_model):
    client, model_name = client_and_model
    proverbs_context = _format_proverbs_context(docs)

    system = (
        "You are a Thai wisdom assistant. "
        """‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:
        - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 ‡∏™‡∏∏‡∏†‡∏≤‡∏©‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏õ‡πá‡∏ô '‡∏™‡∏∏‡∏†‡∏≤‡∏©‡∏¥‡∏ï‡∏à‡∏µ‡∏ô‡πÅ‡∏õ‡∏•‡πÑ‡∏ó‡∏¢' ‡∏ó‡∏µ‡πà‡∏Ñ‡∏°‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏Å‡∏ß‡∏°‡∏ñ‡∏π‡∏Å‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏ß‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
        - ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô explanation ‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤‡∏ß 1‚Äì2 ‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤ (‡∏£‡∏ß‡∏° 4‚Äì8 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ) ‡πÇ‡∏î‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö:
        1) ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏Å‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏û‡∏π‡∏î (‡∏™‡∏±‡πâ‡∏ô ‡πÜ 1‚Äì2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ)
        2) ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ä‡∏±‡∏î ‡πÜ ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡∏™‡∏∏‡∏†‡∏≤‡∏©‡∏¥‡∏ï‡∏ô‡∏µ‡πâ‡∏ñ‡∏∂‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¥‡∏ô‡∏≠‡∏¥‡∏ô
        3) ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ç‡∏≠‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏©‡∏¥‡∏ï‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
        4) ‡∏õ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ ‚Äú‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ 1 ‡∏Ç‡πâ‡∏≠‚Äù (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏¢‡∏≤‡∏ß)"""
        "Return ONLY a valid JSON object with EXACTLY these keys: "
        "\"thai_proverb\" (string), \"explanation\" (string). "
        "No extra keys. No markdown. No code fences."
    )

    user = f"""
        ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏û‡∏π‡∏î:
        {user_situation}

        ‡∏™‡∏∏‡∏†‡∏≤‡∏©‡∏¥‡∏ï‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡πÄ‡∏à‡∏≠ (‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢):
        {proverbs_context}

        ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ:
        {{"thai_proverb":"...","explanation":"..."}}
"""

    try:
        completion = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "system", "content": system},
                      {"role": "user", "content": user}],
            response_format={"type": "json_object"},
            temperature=0,
            max_completion_tokens=1200,
        )
    except Exception:
        completion = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "system", "content": system},
                      {"role": "user", "content": user}],
            temperature=0,
            max_completion_tokens=1200,
        )

    content = completion.choices[0].message.content or ""
    raw = content.strip()

    # 1) ‡∏•‡∏≠‡∏á parse ‡∏ï‡∏£‡∏á ‡πÜ
    try:
        data = json.loads(raw)
    except json.JSONDecodeError:
        # 2) ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà JSON ‡∏•‡πâ‡∏ß‡∏ô ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á JSON object ‡∏Å‡πâ‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ parse
        chunk = _extract_first_json_obj(raw)
        data = json.loads(chunk) if chunk else {}

    thai_proverb = str(data.get("thai_proverb", "")).strip()
    explanation = str(data.get("explanation", "")).strip()

    # ‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ key ‡∏´‡∏≤‡∏¢/‡∏ß‡πà‡∏≤‡∏á: ‡∏™‡πà‡∏á raw ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏µ‡∏ö‡∏±‡∏Å‡πÅ‡∏ó‡∏ô ‚Äú‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‚Äù
    if not thai_proverb or not explanation:
        return {
            "thai_proverb": thai_proverb or "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö (‡∏î‡∏π raw ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)",
            "explanation": explanation or raw or "LLM ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤",
        }

    return {"thai_proverb": thai_proverb, "explanation": explanation}



async def text_to_speech_edge_tts(text: str) -> bytes:
    """
    Convert text to speech using Microsoft Edge TTS (Neural Voice Thai).
    Returns: MP3 bytes
    
    Voice options:
    - "th-TH-PremwadeeNeural" (female, recommended)
    - "th-TH-NiwatNeural" (female, alternative)
    """
    import edge_tts
    #  th-TH-NiwatNeural, th-TH-PremwadeeNeural 
    voice = "th-TH-NiwatNeural"
    communicate = edge_tts.Communicate(text, voice)
    audio_data = b""

    async for chunk in communicate.stream():
        if chunk["type"] == "audio":
            audio_data += chunk["data"]

    return audio_data


# =====================================================
# Streamlit UI
# =====================================================
def main():
    st.title("üé§ Chinese Proverbs Wisdom")
    st.caption(
        "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‚Üí ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏∏‡∏†‡∏≤‡∏©‡∏¥‡∏ï ‚Üí "
        "‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏©‡∏¥‡∏ï‡πÑ‡∏ó‡∏¢ + ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ + ‡∏≠‡πà‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ üîä"
    )
    st.divider()

    # =====================================================
    # Sidebar
    # =====================================================
    with st.sidebar:
        st.header("‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
        
        # Model selection
        st.subheader("üß† ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Speech-to-Text Model")
        model_options = {
            "Thai Whisper large (recommended)": "biodatlab/whisper-th-large-v3-combined",
            "OpenAI Whisper Small": "openai/whisper-small",
            "OpenAI Whisper Base": "openai/whisper-base",
        }
        selected_model = st.selectbox(
            "Model",
            options=model_options.keys(),
            index=0,
            help="‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Speech-to-Text model ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà"
        )
        selected_model_name = model_options[selected_model]

        if "models_loaded" not in st.session_state:
            st.session_state.models_loaded = False

        if st.button("üîÑ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•", type="primary", use_container_width=True):
            try:
                st.session_state.whisper_pipe = load_whisper_model(selected_model_name)
                st.session_state.vector_store, st.session_state.retriever = load_vector_store()
                st.session_state.client_and_model = load_llm_client()
                st.session_state.models_loaded = True
                st.session_state.selected_model = selected_model
                st.success(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î {selected_model} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            except Exception as e:
                st.session_state.models_loaded = False
                st.error(f"‚ùå ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:\n{e}")

        if st.session_state.models_loaded:
            st.success(f"‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ({st.session_state.selected_model})")
        else:
            st.warning("‚ö†Ô∏è ‡∏Å‡∏î '‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•' ‡∏Å‡πà‡∏≠‡∏ô")

    # =====================================================
    # Main content (2 columns)
    # =====================================================
    col1, col2 = st.columns([1, 1])

    # ===== Col 1: Audio Input =====
    with col1:
        st.subheader("üéôÔ∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á")
        st.caption("‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡∏Ñ‡πå‡∏Ç‡∏≠‡∏á browser (Windows host ‡∏ö‡∏ô WSL2)")

        audio = st.audio_input(
            "‡∏Å‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á",
            disabled=not st.session_state.models_loaded,
        )

        if audio and st.button(
            "‚ö° ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°",
            type="primary",
            use_container_width=True,
        ):
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á..."):
                st.session_state.transcribed_text = transcribe_audio(
                    audio.getvalue(), st.session_state.whisper_pipe
                )
                st.success("‚úÖ ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")

    # ===== Col 2: Text Input & Processing =====
    with col2:
        st.subheader("üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏≠‡∏î‡πÑ‡∏î‡πâ")

        if "transcribed_text" in st.session_state:
            edited_text = st.text_area(
                "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏î‡πâ",
                value=st.session_state.transcribed_text,
                height=110,
            ).strip()

            if st.button("üîç ‡∏Ç‡∏≠‡∏™‡∏∏‡∏†‡∏≤‡∏©‡∏¥‡∏ï", type="primary", use_container_width=True):
                if not edited_text:
                    st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πà‡∏≠‡∏ô")
                else:
                    with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏∏‡∏†‡∏≤‡∏©‡∏¥‡∏ï..."):
                        docs = retrieve_proverbs(edited_text, st.session_state.retriever)

                    with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå..."):
                        result = llm_structured_proverb(
                            edited_text,
                            docs,
                            st.session_state.client_and_model,
                        )

                    st.session_state.final_result = result

        else:
            st.info("üëà ‡∏≠‡∏±‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô")

    # =====================================================
    # Final result display
    # =====================================================
    if "final_result" in st.session_state:
        st.divider()
        r = st.session_state.final_result

        thai_proverb = (r.get("thai_proverb") or "").strip()
        explanation = (r.get("explanation") or "").strip()

        if not thai_proverb or not explanation:
            st.error("‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö")
            st.json(r)
            return

        # Display proverb and explanation
        st.markdown("""
            <style>
            .big-explanation {
            font-size: 20px;
            line-height: 1.7;
            }
            </style>
            """, unsafe_allow_html=True)

        st.markdown(f"## ‚ú® {thai_proverb}")

        safe = html.escape(explanation).replace("\n", "<br>")
        st.markdown(f"<div class='big-explanation'>{safe}</div>", unsafe_allow_html=True)

        # TTS button
        if st.button("üîä ‡∏≠‡πà‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤", use_container_width=True):
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á..."):
                try:
                    audio_bytes = asyncio.run(text_to_speech_edge_tts(explanation))
                    st.audio(audio_bytes, format="audio/mp3")
                    st.success("‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
                except Exception as e:
                    st.error(f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:\n{e}")


if __name__ == "__main__":
    if "models_loaded" not in st.session_state:
        st.session_state.models_loaded = False
    main()